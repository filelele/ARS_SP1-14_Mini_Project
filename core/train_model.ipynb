{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = {'background': 0, 'dilmah': 1, 'g7': 2, 'jack-jill': 3, 'karo': 4, 'nestea_atiso': 5, 'nestea_chanh': 6, 'nestea_hoaqua': 7, 'orion': 8, 'tipo': 9, 'y4': 10}\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "class MiniPrjDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = encode  # Use the provided encode dictionary\n",
    "        for cls_name in self.class_to_idx.keys():\n",
    "            \n",
    "            cls_dir = os.path.join(root_dir, cls_name)\n",
    "            print(f\"Checking directory: {cls_dir}\")  # Debug line\n",
    "            if os.path.isdir(cls_dir):\n",
    "                for img_name in os.listdir(cls_dir):\n",
    "                    img_path = os.path.join(cls_dir, img_name)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Dataset object from data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking directory: ..\\frames_224\\total_data_labelled\\background\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\dilmah\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\g7\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\jack-jill\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\karo\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\nestea_atiso\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\nestea_chanh\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\nestea_hoaqua\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\orion\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\tipo\n",
      "Checking directory: ..\\frames_224\\total_data_labelled\\y4\n"
     ]
    }
   ],
   "source": [
    "# Use all dataset for training\n",
    "total_datatransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Replace with data path if needed\n",
    "all_dataset = MiniPrjDataset(root_dir=r'..\\frames_224\\total_data_labelled' , transform=total_datatransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21717"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create class weights\n",
    "This class weight will help rebalance the contributions to loss from loss of each class (Data is imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples in each class: [396, 2725, 2637, 1734, 1577, 2387, 2036, 2233, 2743, 1527, 1722]\n",
      "Class weights: tensor([0.0500, 0.0493, 0.0510, 0.0775, 0.0852, 0.0563, 0.0660, 0.0602, 0.0490,\n",
      "        0.0880, 0.0781])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class_counts = [sum(1 for label in all_dataset.labels if label == i) for i in range(11)]\n",
    "print(f\"Number of data samples in each class: {class_counts}\")\n",
    "\n",
    "total_samples = len(all_dataset)\n",
    "class_weight = torch.tensor([total_samples / count if count > 0 else 0 for count in class_counts], dtype=torch.float32)\n",
    "class_weight = F.normalize(class_weight, p=1, dim=0)\n",
    "class_weight[0] = 0.05\n",
    "print(f\"Class weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create Dataloader for SGD\n",
    "\n",
    "Data samples are images, which will be heavy on device memory and cannot be loaded all at once\n",
    "\n",
    "SGD with small batch size is required\n",
    "\n",
    "Adjust batch size according to train machine memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trainloader = DataLoader(all_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet50_Lite(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, pretrained=True):\n",
    "        super(Resnet50_Lite, self).__init__()\n",
    "        self.resnet50_features = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        self.resnet50_features.fc = nn.Identity() # [512,1]\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet50_features(x)\n",
    "        classification = self.classification_head(features)\n",
    "\n",
    "        return classification  # Return in the desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_train(model, train_loader, epochs, optimizer, criterion, device, epoch_debug=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Use for final model train\n",
    "        total_cost: List of total loss per epoch.\n",
    "        classify_cost: List of classification loss per epoch.\n",
    "        objectbackground_cost: List of object/background loss per epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # Track costs across all epochs\n",
    "    total_cost, classify_cost, objectbackground_cost = [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_cost_epoch, classify_cost_epoch, objectbackground_cost_epoch = 0, 0, 0\n",
    "\n",
    "        for (data, label) in train_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            data = data.squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            class_logits = model(data)\n",
    "            # print(class_logits.shape)\n",
    "            # print(f\"Label shape: {label.shape}, Label type: {type(label)}\")  # Inside __getitem__\n",
    "            loss = criterion(class_logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_cost_epoch += loss.item()\n",
    "\n",
    "        # Log training losses\n",
    "        total_cost.append(total_cost_epoch / len(train_loader))\n",
    "\n",
    "        if epoch_debug:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}: \"\n",
    "                f\"Train Total Loss: {total_cost[-1]:.4f}, \"\n",
    "            )\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train the model\n",
    "Class weight is used here as stated, choose crossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\py39_with_cuda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\py39_with_cuda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Total Loss: 0.1571, \n",
      "Epoch 2/10: Train Total Loss: 0.0059, \n",
      "Epoch 3/10: Train Total Loss: 0.0033, \n",
      "Epoch 4/10: Train Total Loss: 0.0022, \n",
      "Epoch 5/10: Train Total Loss: 0.0017, \n",
      "Epoch 6/10: Train Total Loss: 0.0015, \n",
      "Epoch 7/10: Train Total Loss: 0.0011, \n",
      "Epoch 8/10: Train Total Loss: 0.0009, \n",
      "Epoch 9/10: Train Total Loss: 0.0011, \n",
      "Epoch 10/10: Train Total Loss: 0.0007, \n"
     ]
    }
   ],
   "source": [
    "lite50_model = Resnet50_Lite(num_classes=11, pretrained=True)\n",
    "for params in lite50_model.resnet50_features.parameters():\n",
    "    params.requires_grad = False\n",
    "for params in lite50_model.resnet50_features.layer4.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "class_weight = class_weight.to(device)\n",
    "criterion_lite50 = nn.CrossEntropyLoss(weight=class_weight)\n",
    "optimizer = optim.SGD(lite50_model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00005)\n",
    "\n",
    "lite50_cost = last_train(\n",
    "    lite50_model, total_trainloader, epochs=10, optimizer=optimizer, criterion=criterion_lite50, device=device, epoch_debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lite50_model, '..\\model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple pipeline to apply model on a single video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Apply model on a single video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getBbox\n",
    "import myutils\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "### Test on video\n",
    "def boxAndClass_singleFrame(img, model):\n",
    "    \n",
    "    # To size 224x224\n",
    "    resized_frame, x_pixels_pad = myutils.crop_and_resize(img)\n",
    "\n",
    "    # Get bbox\n",
    "    xmin, ymin, w, h = getBbox.getBbox(resized_frame)\n",
    "    xmin_ratio = xmin / 224\n",
    "    ymin_ratio = ymin / 224\n",
    "    w_ratio = w / 224\n",
    "    h_ratio = h / 224\n",
    "\n",
    "    # Necessary transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    resized_frame = transform(resized_frame).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get model prediction\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(resized_frame)\n",
    "    \n",
    "    # Get predicted class\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    predicted_class = predicted_class.item()\n",
    "\n",
    "    # Map the predicted class to the class name\n",
    "    class_name = {v: k for k, v in encode.items()}[predicted_class]\n",
    "\n",
    "    return class_name, (xmin_ratio, ymin_ratio, w_ratio, h_ratio), x_pixels_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply model on all frames of video, also count the number of object belonging to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxAndClass_video(video_path, model, output_path, persistence_threshold=16):\n",
    "    \"\"\"\n",
    "    Processes a video to predict the bounding box and class label for each frame and saves the output video,\n",
    "    while counting the appearance of each class, using persistence tracking.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the input video.\n",
    "        model (torch.nn.Module): The pre-trained model used for prediction.\n",
    "        output_path (str): The path to save the output video with bounding boxes and class labels.\n",
    "        persistence_threshold (int): The number of frames an object must persist to be counted.\n",
    "    \"\"\"\n",
    "    # Initialize counters for each class\n",
    "    counter = {'dilmah': 0, 'g7': 0, 'jack-jill': 0, 'karo': 0, 'nestea_atiso': 0,\n",
    "               'nestea_chanh': 0, 'nestea_hoaqua': 0, 'orion': 0, 'tipo': 0, 'y4': 0}\n",
    "\n",
    "    # Track the persistence of each object over frames\n",
    "    object_tracker = {'dilmah': 0, 'g7': 0, 'jack-jill': 0, 'karo': 0, 'nestea_atiso': 0,\n",
    "                      'nestea_chanh': 0, 'nestea_hoaqua': 0, 'orion': 0, 'tipo': 0, 'y4': 0, 'background': 0}\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Read until video is completed\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            saved_frame = frame.copy()\n",
    "\n",
    "            # Get the predicted class and bounding box\n",
    "            class_name, bbox, x_pixels_pad = boxAndClass_singleFrame(frame, model)\n",
    "            xmin_ratio, ymin_ratio, w_ratio, h_ratio = bbox\n",
    "            \n",
    "            # Sửa lỗi vẽ box ở ảnh background\n",
    "            if w_ratio < 0.2 or h_ratio < 0.2:\n",
    "                class_name = 'background'\n",
    "            \n",
    "            object_tracker[class_name] += 1\n",
    "\n",
    "            # Display the class name on the frame\n",
    "            cv2.putText(saved_frame, class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            if class_name != 'background':\n",
    "                # Khi gặp object, buffer đếm background đưa về 0\n",
    "                object_tracker['background'] = 0\n",
    "                top_left, bottom_right = myutils.topLeftBottomRight(xmin_ratio, ymin_ratio, w_ratio, h_ratio, x_pixels_pad, frame_height)\n",
    "                saved_frame = cv2.rectangle(saved_frame, top_left, bottom_right, (0, 255, 0), 1)\n",
    "\n",
    "            elif class_name == 'background' and object_tracker['background'] >= 20:\n",
    "                \"\"\"\n",
    "                Gặp quá nhiều background -> kết thúc object => đếm class xuất hiện nhiều nhất => tăng counter cho class đó\n",
    "                \"\"\"\n",
    "                object_tracker['background'] = 0\n",
    "                # Nếu class là background, tìm class có giá trị lớn nhất trong object_tracker\n",
    "                max_class = max(object_tracker, key=object_tracker.get)\n",
    "\n",
    "                # Nếu object xuất hiện đủ nhiều trong object tracker thì tăng counter cho class đó\n",
    "                if object_tracker[max_class] >= persistence_threshold:\n",
    "                    counter[max_class] += 1\n",
    "                \n",
    "                # Reset the object tracker\n",
    "                object_tracker = {key: 0 for key in object_tracker}\n",
    "\n",
    "            # Display the counter at the bottom right corner\n",
    "            counter_text = ', '.join([f'{key}: {value}' for key, value in counter.items()])\n",
    "            text_position = (10, frame_height - 10)  # Adjust text position\n",
    "            cv2.putText(saved_frame, counter_text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Write the frame into the output video\n",
    "            out.write(saved_frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Output the final class counts\n",
    "    print(\"Class counts in video:\")\n",
    "    for class_name, count in counter.items():\n",
    "        print(f\"{class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done! Test on test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts in video:\n",
      "dilmah: 2\n",
      "g7: 1\n",
      "jack-jill: 3\n",
      "karo: 2\n",
      "nestea_atiso: 1\n",
      "nestea_chanh: 0\n",
      "nestea_hoaqua: 1\n",
      "orion: 1\n",
      "tipo: 0\n",
      "y4: 3\n"
     ]
    }
   ],
   "source": [
    "# Test the function on test video\n",
    "boxAndClass_video(r'C:\\Users\\Admin\\Documents\\[ARS_SP1]14_Mini_Project_1\\raw_data\\video_test_1.avi', lite50_model, 'output_video_lite50_model.avi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_with_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
